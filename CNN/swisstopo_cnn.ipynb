{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, shutil\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt # plotting library\n",
    "%matplotlib inline\n",
    "#matplotlib.get_backend()\n",
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU installed - You are as slow as a High-End Windows PC\n"
     ]
    }
   ],
   "source": [
    "if tf.test.gpu_device_name(): \n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "   print(\"No GPU installed - You are as slow as a High-End Windows PC\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_folders = False\n",
    "\n",
    "# Odrner für die Originaldaten\n",
    "original_dataset_dir = 'dataset/original_data'\n",
    "\n",
    "# Ordner für die Trainings, Validierungs und Testdaten\n",
    "base_dir = 'dataset/dataset'\n",
    "#os.mkdir(base_dir)\n",
    "\n",
    "#creates train-, validation- and test-folder\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "#os.mkdir(train_dir)\n",
    "\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "#os.mkdir(validation_dir)\n",
    "\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "#os.mkdir(test_dir)\n",
    "\n",
    "# creates a cat and a dog folder in the train folder\n",
    "train_y_dir = os.path.join(train_dir, 'y')\n",
    "#os.mkdir(train_y_dir)\n",
    "train_n_dir = os.path.join(train_dir, 'n')\n",
    "#os.mkdir(train_n_dir)\n",
    "\n",
    "# creates a cat and a dog folder in the validation folder\n",
    "validation_y_dir = os.path.join(validation_dir, 'y')\n",
    "#os.mkdir(validation_y_dir)\n",
    "validation_n_dir = os.path.join(validation_dir, 'n')\n",
    "#os.mkdir(validation_n_dir)\n",
    "\n",
    "# creates a cat and a dog folder in the test folder\n",
    "test_y_dir = os.path.join(test_dir, 'y')\n",
    "#os.mkdir(test_y_dir)\n",
    "test_n_dir = os.path.join(test_dir, 'n')\n",
    "#os.mkdir(test_n_dir)\n",
    "\n",
    "if new_folders == True:\n",
    "    os.mkdir(base_dir)\n",
    "    os.mkdir(train_dir)\n",
    "    os.mkdir(validation_dir)\n",
    "    os.mkdir(test_dir)\n",
    "    os.mkdir(train_y_dir)\n",
    "    os.mkdir(train_n_dir)\n",
    "    os.mkdir(validation_y_dir)\n",
    "    os.mkdir(validation_n_dir)\n",
    "    os.mkdir(test_y_dir)\n",
    "    os.mkdir(test_n_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total y images: 1667\n",
      "total n images: 1943\n"
     ]
    }
   ],
   "source": [
    "# read size of the dataset folders\n",
    "\n",
    "print('total y images:', len(os.listdir('dataset/original_data/y')))\n",
    "print('total n images:', len(os.listdir('dataset/original_data/n')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculates the number of images for train, validation and test\n",
    "train_size = 0.6\n",
    "validation_size = 0.2\n",
    "test_size = 0.2\n",
    "\n",
    "# calculates the number of images for train, validation and test\n",
    "train_y_size = int(len(os.listdir('dataset/original_data/y')) * train_size)\n",
    "train_n_size = int(len(os.listdir('dataset/original_data/n')) * train_size)\n",
    "\n",
    "validation_y_size = int(len(os.listdir('dataset/original_data/y')) * validation_size)\n",
    "validation_n_size = int(len(os.listdir('dataset/original_data/n')) * validation_size)\n",
    "\n",
    "test_y_size = int(len(os.listdir('dataset/original_data/y')) * test_size)\n",
    "test_n_size = int(len(os.listdir('dataset/original_data/n')) * test_size)\n",
    "\n",
    "train_y_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed = True\n",
    "y_dir = 'dataset/original_data/y/'\n",
    "n_dir = 'dataset/original_data/n/'\n",
    "if renamed == False:\n",
    "    # change name of the images in the dataset folder\n",
    "    # nur einmal Ausführen!\n",
    "\n",
    "\n",
    "\n",
    "    i = 0\n",
    "    for filename in os.listdir(y_dir):\n",
    "        os.rename(y_dir + filename, y_dir + 'y.' + str(i) + '.jpg')\n",
    "        i = i + 1\n",
    "\n",
    "    y = 0\n",
    "    for filename in os.listdir(n_dir):\n",
    "        os.rename(n_dir + filename, n_dir + 'n.' + str(y) + '.jpg')\n",
    "        y = y + 1\n",
    "        \n",
    "\n",
    "    # copy train_y_size images from the original dataset to the train_y_dir\n",
    "    fnames = ['y.{}.jpg'.format(i) for i in range(train_y_size)]\n",
    "    for fname in fnames:\n",
    "        src = os.path.join(y_dir, fname)\n",
    "        dst = os.path.join(train_y_dir, fname)\n",
    "        shutil.copyfile(src, dst)\n",
    "\n",
    "    # copy train_n_size images from the original dataset to the train_n_dir\n",
    "    fnames = ['n.{}.jpg'.format(i) for i in range(train_n_size)]\n",
    "    for fname in fnames:\n",
    "        src = os.path.join(n_dir, fname)\n",
    "        dst = os.path.join(train_n_dir, fname)\n",
    "        shutil.copyfile(src, dst)\n",
    "\n",
    "    # copy validation_y_size images from the original dataset to the validation_y_dir\n",
    "    fnames = ['y.{}.jpg'.format(i) for i in range(train_y_size, train_y_size + validation_y_size)]\n",
    "    for fname in fnames:\n",
    "        src = os.path.join(y_dir, fname)\n",
    "        dst = os.path.join(validation_y_dir, fname)\n",
    "        shutil.copyfile(src, dst)\n",
    "\n",
    "    # copy validation_n_size images from the original dataset to the validation_n_dir\n",
    "    fnames = ['n.{}.jpg'.format(i) for i in range(train_n_size, train_n_size + validation_n_size)]\n",
    "    for fname in fnames:\n",
    "        src = os.path.join(n_dir, fname)\n",
    "        dst = os.path.join(validation_n_dir, fname)\n",
    "        shutil.copyfile(src, dst)\n",
    "\n",
    "    # copy test_y_size images from the original dataset to the test_y_dir\n",
    "    fnames = ['y.{}.jpg'.format(i) for i in range(train_y_size + validation_y_size, train_y_size + validation_y_size + test_y_size)]\n",
    "    for fname in fnames:\n",
    "        src = os.path.join(y_dir, fname)\n",
    "        dst = os.path.join(test_y_dir, fname)\n",
    "        shutil.copyfile(src, dst)\n",
    "\n",
    "    # copy test_n_size images from the original dataset to the test_n_dir\n",
    "    fnames = ['n.{}.jpg'.format(i) for i in range(train_n_size + validation_n_size, train_n_size + validation_n_size + test_n_size)]\n",
    "    for fname in fnames:\n",
    "        src = os.path.join(n_dir, fname)\n",
    "        dst = os.path.join(test_n_dir, fname)\n",
    "        shutil.copyfile(src, dst)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training y images: 1000\n",
      "total training n images: 1165\n",
      "total validation y images: 333\n",
      "total validation n images: 388\n",
      "total test y images: 333\n",
      "total test n images: 388\n"
     ]
    }
   ],
   "source": [
    "print('total training y images:', len(os.listdir(train_y_dir)))\n",
    "print('total training n images:', len(os.listdir(train_n_dir)))\n",
    "print('total validation y images:', len(os.listdir(validation_y_dir)))\n",
    "print('total validation n images:', len(os.listdir(validation_n_dir)))\n",
    "print('total test y images:', len(os.listdir(test_y_dir)))\n",
    "print('total test n images:', len(os.listdir(test_n_dir)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Gathering with Generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2165 images belonging to 2 classes.\n",
      "Found 721 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# without Data Augmentation\n",
    "\n",
    "train_datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "test_datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size = (150, 150),\n",
    "    batch_size = 16,\n",
    "    class_mode = 'binary'\n",
    ")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size = (150,150),\n",
    "    batch_size = 16,\n",
    "    class_mode = 'binary'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the output of one of these generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_batch shape: (16, 150, 150, 3)\n",
      "labels batch shape (16,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for data_batch, labels_batch in train_generator:\n",
    "    print('data_batch shape:', data_batch.shape)\n",
    "    print('labels batch shape', labels_batch.shape)\n",
    "    break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "Nun sollen Sie ein Modell für diese Daten bauen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='acc', patience=10, verbose=1, mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 148, 148, 32)      896       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 148, 148, 32)      0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 146, 146, 64)      18496     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 146, 146, 64)      0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 73, 73, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 341056)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               87310592  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 87,330,241\n",
      "Trainable params: 87,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#create a base_line model\n",
    "\n",
    "base_model = keras.models.Sequential()\n",
    "\n",
    "base_model.add(keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)))\n",
    "base_model.add(keras.layers.Dropout(0.3))\n",
    "base_model.add(keras.layers.Conv2D(64, (3,3), activation='relu'))\n",
    "base_model.add(keras.layers.Dropout(0.3))\n",
    "base_model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "\n",
    "\n",
    "\n",
    "base_model.add(tf.keras.layers.Flatten())\n",
    "base_model.add(keras.layers.Dense(256, activation='relu'))\n",
    "\n",
    "base_model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "base_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "32/32 [==============================] - 58s 2s/step - loss: 8.6980 - acc: 0.5605 - val_loss: 0.6664 - val_acc: 0.5078\n",
      "Epoch 2/30\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.6631 - acc: 0.6707 - val_loss: 0.6647 - val_acc: 0.4922\n",
      "Epoch 3/30\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.7227 - acc: 0.6707 - val_loss: 0.6700 - val_acc: 0.4551\n",
      "Epoch 4/30\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.5817 - acc: 0.6966 - val_loss: 0.6648 - val_acc: 0.4668\n",
      "Epoch 5/30\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.5478 - acc: 0.7422 - val_loss: 0.6406 - val_acc: 0.5449\n",
      "Epoch 6/30\n",
      "32/32 [==============================] - 57s 2s/step - loss: 0.4527 - acc: 0.7910 - val_loss: 0.5680 - val_acc: 0.6660\n",
      "Epoch 7/30\n",
      "32/32 [==============================] - 57s 2s/step - loss: 0.4631 - acc: 0.8242 - val_loss: 0.5816 - val_acc: 0.7266\n",
      "Epoch 8/30\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.4361 - acc: 0.8301 - val_loss: 0.8502 - val_acc: 0.5938\n",
      "Epoch 9/30\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.4804 - acc: 0.8301 - val_loss: 0.5616 - val_acc: 0.7422\n",
      "Epoch 10/30\n",
      "32/32 [==============================] - 58s 2s/step - loss: 0.4230 - acc: 0.8496 - val_loss: 0.7600 - val_acc: 0.6172\n",
      "Epoch 11/30\n",
      "32/32 [==============================] - 57s 2s/step - loss: 0.3953 - acc: 0.8672 - val_loss: 0.7071 - val_acc: 0.6523\n",
      "Epoch 12/30\n",
      "32/32 [==============================] - 57s 2s/step - loss: 0.4506 - acc: 0.8503 - val_loss: 0.7210 - val_acc: 0.6445\n",
      "Epoch 13/30\n",
      "32/32 [==============================] - 58s 2s/step - loss: 0.3491 - acc: 0.8809 - val_loss: 0.7283 - val_acc: 0.6953\n",
      "Epoch 14/30\n",
      "32/32 [==============================] - 57s 2s/step - loss: 0.3199 - acc: 0.9082 - val_loss: 0.6673 - val_acc: 0.6973\n",
      "Epoch 15/30\n",
      "32/32 [==============================] - 58s 2s/step - loss: 0.3266 - acc: 0.8848 - val_loss: 1.5627 - val_acc: 0.5664\n",
      "Epoch 16/30\n",
      "32/32 [==============================] - 58s 2s/step - loss: 0.3319 - acc: 0.9023 - val_loss: 1.0549 - val_acc: 0.6816\n",
      "Epoch 17/30\n",
      "32/32 [==============================] - 58s 2s/step - loss: 0.2260 - acc: 0.9142 - val_loss: 0.5592 - val_acc: 0.7988\n",
      "Epoch 18/30\n",
      "32/32 [==============================] - 57s 2s/step - loss: 0.1959 - acc: 0.9355 - val_loss: 0.7359 - val_acc: 0.7344\n",
      "Epoch 19/30\n",
      "32/32 [==============================] - 58s 2s/step - loss: 0.2911 - acc: 0.9180 - val_loss: 0.7634 - val_acc: 0.6875\n",
      "Epoch 20/30\n",
      "32/32 [==============================] - 58s 2s/step - loss: 0.1688 - acc: 0.9492 - val_loss: 0.6569 - val_acc: 0.7246\n",
      "Epoch 21/30\n",
      "32/32 [==============================] - 57s 2s/step - loss: 0.2281 - acc: 0.9121 - val_loss: 0.8686 - val_acc: 0.7109\n",
      "Epoch 22/30\n",
      "32/32 [==============================] - 57s 2s/step - loss: 0.2274 - acc: 0.9242 - val_loss: 0.6757 - val_acc: 0.7305\n",
      "Epoch 23/30\n",
      "32/32 [==============================] - 59s 2s/step - loss: 0.2222 - acc: 0.9414 - val_loss: 0.4985 - val_acc: 0.8164\n",
      "Epoch 24/30\n",
      "32/32 [==============================] - 58s 2s/step - loss: 0.1409 - acc: 0.9512 - val_loss: 0.5488 - val_acc: 0.8203\n",
      "Epoch 25/30\n",
      "32/32 [==============================] - 59s 2s/step - loss: 0.1586 - acc: 0.9375 - val_loss: 0.6880 - val_acc: 0.7715\n",
      "Epoch 26/30\n",
      "32/32 [==============================] - 58s 2s/step - loss: 0.1941 - acc: 0.9492 - val_loss: 1.0067 - val_acc: 0.6602\n",
      "Epoch 27/30\n",
      "32/32 [==============================] - 58s 2s/step - loss: 0.1685 - acc: 0.9541 - val_loss: 1.5794 - val_acc: 0.6367\n",
      "Epoch 28/30\n",
      "32/32 [==============================] - 58s 2s/step - loss: 0.1201 - acc: 0.9648 - val_loss: 1.2482 - val_acc: 0.7012\n",
      "Epoch 29/30\n",
      "32/32 [==============================] - 57s 2s/step - loss: 0.1197 - acc: 0.9561 - val_loss: 0.6364 - val_acc: 0.8223\n",
      "Epoch 30/30\n",
      "32/32 [==============================] - 57s 2s/step - loss: 0.1421 - acc: 0.9648 - val_loss: 0.8947 - val_acc: 0.7500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Compile the model with balanced class weights\n",
    "base_model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.RMSprop(learning_rate=1e-3), metrics=['acc'])\n",
    "\n",
    "# Train the model with balanced class weights\n",
    "history = base_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=32,\n",
    "    epochs=30,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=32\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Assuming you have defined the variables 'acc' and 'val_acc'\n",
    "#title\n",
    "fig.update_layout(\n",
    "    title=\"Base Model\",\n",
    "    xaxis_title=\"Epochs\",\n",
    "    yaxis_title=\"Accuracy\",\n",
    "    font=dict(\n",
    "        family=\"Courier New, monospace\",\n",
    "        size=18,\n",
    "        color=\"#7f7f7f\"\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "fig.add_trace(go.Scatter(x=np.arange(1, len(acc)+1), y=acc,\n",
    "                    mode='lines',\n",
    "                    name='accuracy'))\n",
    "fig.add_trace(go.Scatter(x=np.arange(1, len(val_acc)+1), y=val_acc,\n",
    "                    mode='lines',\n",
    "                    name='validation accuracy'))\n",
    "\n",
    "# Save the plot as an HTML file\n",
    "pio.write_html(fig, 'base_model.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 721 images belonging to 2 classes.\n",
      "46/46 [==============================] - 30s 664ms/step - loss: 0.3663 - acc: 0.8877\n",
      "test acc: 0.887656033039093\n",
      "46/46 [==============================] - 27s 587ms/step\n",
      "46/46 - 28s - loss: 0.3663 - acc: 0.8877 - 28s/epoch - 606ms/step\n",
      "\n",
      "Test accuracy: 0.36631548404693604\n",
      "\n",
      "Test loss: 0.887656033039093\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.49      0.51       388\n",
      "           1       0.45      0.49      0.47       333\n",
      "\n",
      "    accuracy                           0.49       721\n",
      "   macro avg       0.49      0.49      0.49       721\n",
      "weighted avg       0.49      0.49      0.49       721\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhsAAAHFCAYAAABb+zt/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAABAtUlEQVR4nO3deVxV1f7/8feR4SAOKKICKTjPhoLmnJKm4sjVMtNuzl6zyRxul7xm1jXUm2OlmUOYetXSRG3wailZqRUolkOGhVOJQ5oEIoLs3x/+PN97ApNjZ3uE83r22I9He+211/4cehgfP2utfSyGYRgCAAAwSQlXBwAAAIo3kg0AAGAqkg0AAGAqkg0AAGAqkg0AAGAqkg0AAGAqkg0AAGAqkg0AAGAqkg0AAGAqkg0UW998842GDBmi6tWry8fHR6VLl1Z4eLhmzJih8+fPm/rsvXv3qn379vLz85PFYtGcOXOc/gyLxaIXXnjB6ePeSV5++WXFx8c7dE9cXJwsFouOHj1qSkwAHGfhdeUojhYtWqTRo0erbt26Gj16tBo0aKCcnBwlJiZq0aJFCgsL0/r16017ftOmTZWZmam5c+eqfPnyqlatmgIDA536jN27d6tKlSqqUqWKU8e9k5QuXVoPPPCA4uLiCn3P2bNn9cMPP6hp06ayWq3mBQeg0Eg2UOzs2rVL7dq10/3336/4+Ph8v3CuXLmizZs3q1evXqbF4OXlpREjRmj+/PmmPcMdOJJsZGVlycfHRxaLxfzAADiEaRQUOy+//LIsFovefPPNAv9m6+3tbZdo5OXlacaMGapXr56sVqsqVaqkRx99VCdPnrS7r0OHDmrUqJG+/vprtWvXTr6+vqpRo4amTZumvLw8Sf9Xws/NzdWCBQtksVhsv/xeeOGFAn8RFlT237Ztmzp06KAKFSqoZMmSCgkJUd++fXXp0iVbn4KmUfbv36/evXurfPny8vHxUZMmTbRs2TK7PgkJCbJYLFq1apUmTpyo4OBglS1bVp06ddLhw4dv+vO9/jm++eYbPfjgg/Lz85O/v7/Gjh2r3NxcHT58WF27dlWZMmVUrVo1zZgxw+7+y5cva9y4cWrSpInt3latWmnDhg12/SwWizIzM7Vs2TLbz7FDhw52P7MtW7Zo6NChqlixonx9fZWdnZ3v55mSkqKyZcvqwQcftBt/27Zt8vDw0KRJk276mQH8OSQbKFauXr2qbdu2KSIiQlWrVi3UPY899pieffZZ3X///dq4caNeeuklbd68Wa1bt9a5c+fs+qalpWngwIF65JFHtHHjRkVFRSkmJkYrVqyQJHXv3l27du2SJD3wwAPatWuX7bywjh49qu7du8vb21tLly7V5s2bNW3aNJUqVUpXrly54X2HDx9W69atdeDAAc2bN0/vvfeeGjRooMGDB+f7hS9Jzz33nI4dO6bFixfrzTffVEpKinr27KmrV68WKs5+/fopLCxM69at04gRIzR79mw988wzio6OVvfu3bV+/Xrdd999evbZZ/Xee+/Z7svOztb58+c1fvx4xcfHa9WqVWrbtq369Omjt99+29Zv165dKlmypLp162b7Of6+UjR06FB5eXlp+fLlWrt2rby8vPLFWbt2bS1atEhr167VvHnzJF377zhgwAC1a9eu2K97Ae4IBlCMpKWlGZKM/v37F6r/oUOHDEnG6NGj7dq//PJLQ5Lx3HPP2drat29vSDK+/PJLu74NGjQwunTpYtcmyXj88cft2iZPnmwU9EfurbfeMiQZqamphmEYxtq1aw1JRnJy8h/GLsmYPHmy7bx///6G1Wo1jh8/btcvKirK8PX1NX799VfDMAxj+/bthiSjW7dudv3eeecdQ5Kxa9euP3zu9c8xc+ZMu/YmTZoYkoz33nvP1paTk2NUrFjR6NOnzw3Hy83NNXJycoxhw4YZTZs2tbtWqlQpY9CgQfnuuf4ze/TRR2947frP87rHHnvM8Pb2Nnbt2mXcd999RqVKlYyff/75Dz8rAOegsgG3tn37dknS4MGD7drvuece1a9fX5988olde2BgoO655x67trvvvlvHjh1zWkxNmjSRt7e3Ro4cqWXLlunHH38s1H3btm1Tx44d81V0Bg8erEuXLuWrsPx+zcrdd98tSYX+LD169LA7r1+/viwWi6Kiomxtnp6eqlWrVr4x3333XbVp00alS5eWp6envLy8tGTJEh06dKhQz76ub9++he47e/ZsNWzYUJGRkUpISNCKFSsUFBTk0PMA3BqSDRQrAQEB8vX1VWpqaqH6//LLL5JU4C+d4OBg2/XrKlSokK+f1WpVVlbWLURbsJo1a+rjjz9WpUqV9Pjjj6tmzZqqWbOm5s6d+4f3/fLLLzf8HNev/6/ff5br61sK+1n8/f3tzr29veXr6ysfH5987ZcvX7adv/fee+rXr5/uuusurVixQrt27dLXX3+toUOH2vUrDEeSBavVqgEDBujy5ctq0qSJ7r//foeeBeDWkWygWPHw8FDHjh2VlJSUb4FnQa7/wj116lS+az///LMCAgKcFtv1X8LZ2dl27b9fFyJJ7dq106ZNm3Tx4kXt3r1brVq10pgxY7R69eobjl+hQoUbfg5JTv0sf8aKFStUvXp1rVmzRtHR0WrZsqWaNWuW7+dSGI7sPNm/f7+ef/55NW/eXHv27NGsWbMcfh6AW0OygWInJiZGhmFoxIgRBS6ozMnJ0aZNmyRJ9913nyTZFnhe9/XXX+vQoUPq2LGj0+KqVq2apGsvG/tf12MpiIeHh1q0aKHXX39dkrRnz54b9u3YsaO2bdtmSy6ue/vtt+Xr66uWLVveYuTOZbFY5O3tbZcopKWl5duNIjmvapSZmakHH3xQ1apV0/bt2/XEE0/oH//4h7788ss/PTaAm/N0dQCAs7Vq1UoLFizQ6NGjFRERoccee0wNGzZUTk6O9u7dqzfffFONGjVSz549VbduXY0cOVKvvvqqSpQooaioKB09elSTJk1S1apV9cwzzzgtrm7dusnf31/Dhg3Tiy++KE9PT8XFxenEiRN2/d544w1t27ZN3bt3V0hIiC5fvqylS5dKkjp16nTD8SdPnqz3339fkZGRev755+Xv76+VK1fqgw8+0IwZM+Tn5+e0z/Jn9OjRQ++9955Gjx6tBx54QCdOnNBLL72koKAgpaSk2PVt3LixEhIStGnTJgUFBalMmTKqW7euw88cNWqUjh8/rq+++kqlSpXSzJkztWvXLvXv31979+5VuXLlnPTpABSEZAPF0ogRI3TPPfdo9uzZmj59utLS0uTl5aU6depowIABeuKJJ2x9FyxYoJo1a2rJkiV6/fXX5efnp65duyo2NrbANRq3qmzZstq8ebPGjBmjRx55ROXKldPw4cMVFRWl4cOH2/o1adJEW7Zs0eTJk5WWlqbSpUurUaNG2rhxozp37nzD8evWraudO3fqueee0+OPP66srCzVr19fb731Vr4FsK40ZMgQnTlzRm+88YaWLl2qGjVq6B//+IdOnjypKVOm2PWdO3euHn/8cfXv31+XLl1S+/btlZCQ4NDzFi9erBUrVuitt95Sw4YNJV1bR7JmzRqFh4dryJAhpr5NFgBvEAUAACZjzQYAADAVyQYAADAVyQYAADAVyQYAADAVyQYAADAVyQYAADAVyQYAADBVsXypV9b2xa4OAbgjeTa+z9UhAHccr4Aapj8j51zhvr35Zm5HrGagsgEAAExVLCsbAADcUfKuujoClyLZAADAbEaeqyNwKZINAADMlufeyQZrNgAAgKmobAAAYDLDzadRqGwAAGC2vDznHA7asWOHevbsqeDgYFksFsXHx9tdP336tAYPHqzg4GD5+vqqa9euSklJsevToUMHWSwWu6N///4OxUGyAQBAMZWZmamwsDC99tpr+a4ZhqHo6Gj9+OOP2rBhg/bu3avQ0FB16tRJmZmZdn1HjBihU6dO2Y6FCxc6FAfTKAAAmM1F0yhRUVGKiooq8FpKSop2796t/fv3q2HDhpKk+fPnq1KlSlq1apWGDx9u6+vr66vAwMBbjoPKBgAAZsu76pQjOztb6enpdkd2dvYthXT9Ph8fH1ubh4eHvL299fnnn9v1XblypQICAtSwYUONHz9ev/32m0PPItkAAKCIiI2NlZ+fn90RGxt7S2PVq1dPoaGhiomJ0YULF3TlyhVNmzZNaWlpOnXqlK3fwIEDtWrVKiUkJGjSpElat26d+vTp49CzLIZhGLcU5R2M70YBCsZ3owD53Y7vG7lyNNEp4xhBjfNVMqxWq6xW603vtVgsWr9+vaKjo21tSUlJGjZsmPbt2ycPDw916tRJJUpcq0N8+OGHBY6TlJSkZs2aKSkpSeHh4YWKmzUbAACYzUkv9SpsYlFYERERSk5O1sWLF3XlyhVVrFhRLVq0ULNmzW54T3h4uLy8vJSSklLoZINpFAAA3Jyfn58qVqyolJQUJSYmqnfv3jfse+DAAeXk5CgoKKjQ41PZAADAZK56qVdGRoaOHDliO09NTVVycrL8/f0VEhKid999VxUrVlRISIi+/fZbPf3004qOjlbnzp0lST/88INWrlypbt26KSAgQAcPHtS4cePUtGlTtWnTptBxkGwAAGA2F303SmJioiIjI23nY8eOlSQNGjRIcXFxOnXqlMaOHavTp08rKChIjz76qCZNmmTr7+3trU8++URz585VRkaGqlatqu7du2vy5Mny8PAodBwsEAXcCAtEgfxuxwLR7O8/v3mnQrDWaeuUcW431mwAAABTMY0CAIDZ8q66OgKXItkAAMBsfOsrAACAeahsAABgNhftRrlTkGwAAGA2plEAAADMQ2UDAACzMY0CAADMZBjuvfWVaRQAAGAqKhsAAJjNzReIkmwAAGA21mwAAABTuXllgzUbAADAVFQ2AAAwG1/EBgAATMU0CgAAgHmobAAAYDZ2owAAAFMxjQIAAGAeKhsAAJiNaRQAAGAqN082mEYBAACmorIBAIDJ3P0r5kk2AAAwm5tPo5BsAABgNra+AgAAmIfKBgAAZmMaBQAAmIppFAAAAPNQ2QAAwGxMowAAAFMxjQIAAGAeKhsAAJiNaRQAAGAqN082mEYBAACmorIBAIDZ3HyBKMkGAABmc/NpFJINAADM5uaVDdZsAAAAU1HZAADAbEyjAAAAUzGNAgAAYB6SDQAAzJaX55zDQTt27FDPnj0VHBwsi8Wi+Ph4u+unT5/W4MGDFRwcLF9fX3Xt2lUpKSl2fbKzs/Xkk08qICBApUqVUq9evXTy5EmH4iDZAADAbC5KNjIzMxUWFqbXXnst3zXDMBQdHa0ff/xRGzZs0N69exUaGqpOnTopMzPT1m/MmDFav369Vq9erc8//1wZGRnq0aOHrl69Wug4WLMBAEAxFRUVpaioqAKvpaSkaPfu3dq/f78aNmwoSZo/f74qVaqkVatWafjw4bp48aKWLFmi5cuXq1OnTpKkFStWqGrVqvr444/VpUuXQsVBZQMAALMZhlOO7Oxspaen2x3Z2dm3FNL1+3x8fGxtHh4e8vb21ueffy5JSkpKUk5Ojjp37mzrExwcrEaNGmnnzp2FfhbJBgAAZnPSNEpsbKz8/PzsjtjY2FsKqV69egoNDVVMTIwuXLigK1euaNq0aUpLS9OpU6ckSWlpafL29lb58uXt7q1cubLS0tIK/SymUQAAKCJiYmI0duxYuzar1XpLY3l5eWndunUaNmyY/P395eHhoU6dOt1w2uV/GYYhi8VS6GeRbAAAYDYnvdTLarXecnJRkIiICCUnJ+vixYu6cuWKKlasqBYtWqhZs2aSpMDAQF25ckUXLlywq26cOXNGrVu3LvRzmEYBAMBsRp5zDpP4+fmpYsWKSklJUWJionr37i3pWjLi5eWlrVu32vqeOnVK+/fvdyjZoLIBAIDZXPS68oyMDB05csR2npqaquTkZPn7+yskJETvvvuuKlasqJCQEH377bd6+umnFR0dbVsQ6ufnp2HDhmncuHGqUKGC/P39NX78eDVu3Ni2O6UwSDYAACimEhMTFRkZaTu/vt5j0KBBiouL06lTpzR27FidPn1aQUFBevTRRzVp0iS7MWbPni1PT0/169dPWVlZ6tixo+Li4uTh4VHoOCyGYRjO+Uh3jqzti10dAnBH8mx8n6tDAO44XgE1TH9G1rJ/OGWckoOmOWWc243KBgAAZnPzb31lgSgAADAVlQ0AAMzm5pUNkg0AAMxm4rbVooBpFAAAYCoqGwAAmMzIK3YbPx1CsgEAgNncfM0G0ygAAMBUVDYAADCbmy8QJdkAAMBsrNkAAACmYs0GAACAeahsAABgNjevbJBsAABgtuL3BesOYRoFAACYisoGHJaUckLLtnytQ8fTdPZipmaNitZ9TWrbrv+Snqk5732q3YeO6rdL2QqvXUXPPtRJoZXLS5IuZmZpwaYvtOvQUZ0+/5vKlS6pyCa1NbpXW5UpaXXVxwL+lMTkb/XWf9bq4HdHdPaX85obO0kd721tu37u/AXNnr9UO7/ao98yMhXRpJGee+YxhVa9y26c5P2HNG/hMn178Dt5enqqbu0aemPmS/Kx8mejSHPzaRQqG3BYVnaO6lSpqH/075TvmmEYembBev107qJmP/YXrZ44SEEVymrU3HeUlX1FknT21wydvZihsX076N3nB+vFQVH64kCqpry9+XZ/FMBpsrIuq26tGnpu7Oh81wzD0NP/eFEnf07TvOnP6923XlNwYCUNf/o5Xcq6bOuXvP+QRo39p1rfE65Vi+Zq9eK5GtC3p0pYLLfzo8AMeYZzjiKKygYc1rZRDbVtVKPAa8fPXNA3qae09vkhqhUcIEl67uH7dd+E1/XR19+pT9u7Veuuipr5t2jbPVUrltcTvdtp4lsfKPdqnjw9yIFR9LRr1VztWjUv8NqxEz9p34HvFL/8DdWqESpJ+ue4x3Vvj4f14dYEPdCrqyRpxtyFGvhAbw3/az/bvb+vfABFkUv/r37y5ElNnDhRkZGRql+/vho0aKDIyEhNnDhRJ06ccGVouEVXcq9KkqxeHrY2jxIl5OXhob1HTt7wvoysbJX28SbRQLF0JSdHkuTt7WVr8/DwkJeXp/Z+c0CS9MuFX/XNwcPyL++ngX8bq3t7PKzBj0/Qnn37XRIznMzIc85RRLns/+yff/656tevr/Xr1yssLEyPPvqoHnnkEYWFhSk+Pl4NGzbUF1984arwcIuqBforyL+s5q3/TOmZl5WTe1VLN3+pc+mZOpeeWeA9v2ZkadGHu9S3Xdhtjha4PaqHVlVwYCXNXRini+m/KScnR4uXv6Nzv1zQ2V/OS5JO/nRKkjR/6Uo90KurFs56SfXr1NKwp2N07MRPrgwfzsA0ims888wzGj58uGbPnn3D62PGjNHXX3/9h+NkZ2crOzvbri3vSo6s//M3CNw+Xh4emvm33nph+WbdO+5VeZSwqEW9ULVpWL3A/hlZ2Xry9XWqEVRBf+vRusA+QFHn5emp2VP/qedj56hNVD95eJRQy2ZN1a5lM1ufvP+/NfLB3t30l+6dJUn169TS7qRkvff+Fj3z2BCXxA44g8uSjf3792vFihU3vP63v/1Nb7zxxk3HiY2N1ZQpU+zannu0p/45uNefjhG3pkFooN7552D9lpWtnNyr8i/jq0emrVCD0Mp2/TIvX9HoV9fK1+qlWaOi5eXhcYMRgaKvYb3aWrfsdf2WkamcnBz5ly+nh0eMUcN613ZyVazgL0mqWT3E7r4aoSFKO33mtscL5zLYjeIaQUFB2rlz5w2v79q1S0FBQTcdJyYmRhcvXrQ7JgyIcmaouEVlSlrlX8ZXx05f0MFjaeoQVst2LSMrW4/NfUdeHh6aM7qPrF6sVYZ7KFO6lPzLl9OxEz/pwHcpimzbUpJ0V1BlVQqooKPH7Nc2HTtxUkGBlQsaCkUJ0yiuMX78eI0aNUpJSUm6//77VblyZVksFqWlpWnr1q1avHix5syZc9NxrFarrL/bf57FFIqpLl2+ouNnL9jOfzp3Ud+dOC2/UiUV5F9WW5IOq3zpa/+e8tNZzXhnmyKb1FLrBtemUjIvX9Fj897V5Ss5mjq0uzKzspWZdW0qrHwZX3mUYJEoip5Ll7J0/OTPtvOffj6t777/QX5lyygosJL+u+0zlS/np6DKFZXy41FNm/OG7mvXSm1aREiSLBaLhgzoq9eXrFDd2tVVr3ZNbfjwY6UeO6lZ/5roqo8FZynCizudwWXJxujRo1WhQgXNnj1bCxcu1NWr13YxeHh4KCIiQm+//bb69et3k1HgCgeOpWnE7DW285lrt0uSerZsqJcGd9O5ixmauXa7fknPVEW/0urRsqFGdmtl63/wWJq+Tb22GK7npMV2Y3/wr5G6K8DvNnwKwLn2f5eioU8+azuf8eqbkqTeUZ009Z/jdPaX85rx6pv65fyvqljBX726dtSoIQ/bjfHXh/6i7Cs5mj7vTaWn/6Y6tWpo0ZypCqkSfFs/C+BsFsNw/Qvbc3JydO7cOUlSQECAvLz+XGUia/vim3cC3JBn4/tcHQJwx/EKKPi9Qc6U+eJAp4xT6vmVThnndrsjJsq9vLwKtT4DAIAiiQWiAAAA5rkjKhsAABRrRXgniTOQbAAAYDY3343CNAoAADAVlQ0AAMzGNAoAADATrysHAAAwEZUNAADMxjQKAAAwFckGAAAwFVtfAQAAzENlAwAAszGNAgAAzGS4ebLBNAoAADAVlQ0AAMzm5pUNkg0AAMzGG0QBAEBxtGPHDvXs2VPBwcGyWCyKj4+3u56RkaEnnnhCVapUUcmSJVW/fn0tWLDArk+HDh1ksVjsjv79+zsUB5UNAADM5qJplMzMTIWFhWnIkCHq27dvvuvPPPOMtm/frhUrVqhatWrasmWLRo8ereDgYPXu3dvWb8SIEXrxxRdt5yVLlnQoDpINAADM5qJkIyoqSlFRUTe8vmvXLg0aNEgdOnSQJI0cOVILFy5UYmKiXbLh6+urwMDAW46DaRQAANxU27ZttXHjRv30008yDEPbt2/X999/ry5dutj1W7lypQICAtSwYUONHz9ev/32m0PPobIBAIDJDMM5lY3s7GxlZ2fbtVmtVlmt1lsab968eRoxYoSqVKkiT09PlShRQosXL1bbtm1tfQYOHKjq1asrMDBQ+/fvV0xMjPbt26etW7cW+jkkGwAAmM1J0yixsbGaMmWKXdvkyZP1wgsv3NJ48+bN0+7du7Vx40aFhoZqx44dGj16tIKCgtSpUydJ19ZrXNeoUSPVrl1bzZo10549exQeHl6o55BsAABgNiclGzExMRo7dqxd261WNbKysvTcc89p/fr16t69uyTp7rvvVnJysl555RVbsvF74eHh8vLyUkpKCskGAADFzZ+ZMvm9nJwc5eTkqEQJ++WbHh4eyvuD94IcOHBAOTk5CgoKKvSzSDYAADCZq74bJSMjQ0eOHLGdp6amKjk5Wf7+/goJCVH79u01YcIElSxZUqGhofr000/19ttva9asWZKkH374QStXrlS3bt0UEBCggwcPaty4cWratKnatGlT6DhINgAAMJuLko3ExERFRkbazq9PwQwaNEhxcXFavXq1YmJiNHDgQJ0/f16hoaGaOnWqRo0aJUny9vbWJ598orlz5yojI0NVq1ZV9+7dNXnyZHl4eBQ6DovhrCWyd5Cs7YtdHQJwR/JsfJ+rQwDuOF4BNUx/xsVBHZ0yjt+yT5wyzu1GZQMAALO591ejkGwAAGA2V63ZuFPwBlEAAGAqKhsAAJjNzSsbJBsAAJjNzddsMI0CAABMRWUDAACTufsCUZINAADM5ubTKCQbAACYzN0rG6zZAAAApqKyAQCA2ZhGAQAAZjLcPNlgGgUAAJiKygYAAGZz88oGyQYAACZjGgUAAMBEVDYAADCbm1c2SDYAADCZu0+jkGwAAGAyd082WLMBAABMRWUDAACTuXtlg2QDAACzGRZXR+BSTKMAAABTUdkAAMBkTKMAAABTGXlMowAAAJiGygYAACZjGqUQ5s2bV+gBn3rqqVsOBgCA4shw890ohUo2Zs+eXajBLBYLyQYAALBTqGQjNTXV7DgAACi23H0a5ZYXiF65ckWHDx9Wbm6uM+MBAKDYMfIsTjmKKoeTjUuXLmnYsGHy9fVVw4YNdfz4cUnX1mpMmzbN6QECAFDUGYZzjqLK4WQjJiZG+/btU0JCgnx8fGztnTp10po1a5waHAAAKPoc3voaHx+vNWvWqGXLlrJY/q+k06BBA/3www9ODQ4AgOKgKE+BOIPDycbZs2dVqVKlfO2ZmZl2yQcAALjG3ZMNh6dRmjdvrg8++MB2fj3BWLRokVq1auW8yAAAQLHgcGUjNjZWXbt21cGDB5Wbm6u5c+fqwIED2rVrlz799FMzYgQAoEgryos7ncHhykbr1q31xRdf6NKlS6pZs6a2bNmiypUra9euXYqIiDAjRgAAijR33/p6S9+N0rhxYy1btszZsQAAgGLolpKNq1evav369Tp06JAsFovq16+v3r17y9OT73UDAOD3+G4UB+3fv1+9e/dWWlqa6tatK0n6/vvvVbFiRW3cuFGNGzd2epAAABRlvK7cQcOHD1fDhg118uRJ7dmzR3v27NGJEyd09913a+TIkWbECAAAijCHKxv79u1TYmKiypcvb2srX768pk6dqubNmzs1OAAAioM8N59GcbiyUbduXZ0+fTpf+5kzZ1SrVi2nBAUAQHFiGBanHEVVoZKN9PR02/Hyyy/rqaee0tq1a3Xy5EmdPHlSa9eu1ZgxYzR9+nSz4wUAoMhx1dbXHTt2qGfPngoODpbFYlF8fLzd9YyMDD3xxBOqUqWKSpYsqfr162vBggV2fbKzs/Xkk08qICBApUqVUq9evXTy5EmH4ijUNEq5cuXsXkVuGIb69etnazP+/9tKevbsqatXrzoUAAAAMEdmZqbCwsI0ZMgQ9e3bN9/1Z555Rtu3b9eKFStUrVo1bdmyRaNHj1ZwcLB69+4tSRozZow2bdqk1atXq0KFCho3bpx69OihpKQkeXh4FCqOQiUb27dvd+CjAQCA/+WqN4hGRUUpKirqhtd37dqlQYMGqUOHDpKkkSNHauHChUpMTFTv3r118eJFLVmyRMuXL1enTp0kSStWrFDVqlX18ccfq0uXLoWKo1DJRvv27Qs1GAAAyM9Zb//Mzs5Wdna2XZvVapXVar2l8dq2bauNGzdq6NChCg4OVkJCgr7//nvNnTtXkpSUlKScnBx17tzZdk9wcLAaNWqknTt3FjrZcHiB6HWXLl3Sd999p2+++cbuAAAA5oiNjZWfn5/dERsbe8vjzZs3Tw0aNFCVKlXk7e2trl27av78+Wrbtq0kKS0tTd7e3nY7UCWpcuXKSktLK/Rzbukr5ocMGaKPPvqowOus2QAAwJ6ztr7GxMRo7Nixdm23WtWQriUbu3fv1saNGxUaGqodO3Zo9OjRCgoKsk2bFMQwDLu1nDfjcLIxZswYXbhwQbt371ZkZKTWr1+v06dP61//+pdmzpzp6HAAABR7ztq2+memTH4vKytLzz33nNavX6/u3btLku6++24lJyfrlVdeUadOnRQYGKgrV67owoULdtWNM2fOqHXr1oV+lsPTKNu2bdPs2bPVvHlzlShRQqGhoXrkkUc0Y8aMP1XKAQAAt09OTo5ycnJUooR9KuDh4aG8vGvvV4+IiJCXl5e2bt1qu37q1Cnt37/foWTD4cpGZmamKlWqJEny9/fX2bNnVadOHTVu3Fh79uxxdDgAAIo9V+1GycjI0JEjR2znqampSk5Olr+/v0JCQtS+fXtNmDBBJUuWVGhoqD799FO9/fbbmjVrliTJz89Pw4YN07hx41ShQgX5+/tr/Pjxaty48R9Os/yew8lG3bp1dfjwYVWrVk1NmjTRwoULVa1aNb3xxhsKCgpydDgAAIo9V72uPDExUZGRkbbz6+s9Bg0apLi4OK1evVoxMTEaOHCgzp8/r9DQUE2dOlWjRo2y3TN79mx5enqqX79+ysrKUseOHRUXF1fod2xIksUwHMu3Vq5cqZycHA0ePFh79+5Vly5d9Msvv8jb21txcXF66KGHHBnOFFnbF7s6BOCO5Nn4PleHANxxvAJqmP6M5NBeThmnybGNThnndnO4sjFw4EDbvzdt2lRHjx7Vd999p5CQEAUEBDg1OAAAioOi/L0mzuBwsvF7vr6+Cg8Pd0YsAAAUS65as3GnKFSy8fs9vX/k+qISAABwjbt/xXyhko29e/cWajBHXvABAADcg8MLRIuCv1V70NUhAHekJT/vdHUIwB0n98pPpj/j67v+4pRxmv+03inj3G5/es0GAAD4Y+4+jXLLX8QGAABQGFQ2AAAwWbFbr+Agkg0AAEzGNAoAAICJbinZWL58udq0aaPg4GAdO3ZMkjRnzhxt2LDBqcEBAFAcGIbFKUdR5XCysWDBAo0dO1bdunXTr7/+qqtXr0qSypUrpzlz5jg7PgAAirw8Jx1FlcPJxquvvqpFixZp4sSJdt/41qxZM3377bdODQ4AABR9Di8QTU1NVdOmTfO1W61WZWZmOiUoAACKE0NFdwrEGRyubFSvXl3Jycn52j/66CM1aNDAGTEBAFCs5BnOOYoqhysbEyZM0OOPP67Lly/LMAx99dVXWrVqlWJjY7V48WIzYgQAoEjLc/PKhsPJxpAhQ5Sbm6u///3vunTpkgYMGKC77rpLc+fOVf/+/c2IEQAAFGG39FKvESNGaMSIETp37pzy8vJUqVIlZ8cFAECx4e5rNv7UG0QDAgKcFQcAAMVWUd626gwOJxvVq1eXxXLjDO3HH3/8UwEBAIDixeFkY8yYMXbnOTk52rt3rzZv3qwJEyY4Ky4AAIoNplEc9PTTTxfY/vrrrysxMfFPBwQAQHHj7tMoTvsitqioKK1bt85ZwwEAgGLCaV8xv3btWvn7+ztrOAAAig13r2w4nGw0bdrUboGoYRhKS0vT2bNnNX/+fKcGBwBAccCaDQdFR0fbnZcoUUIVK1ZUhw4dVK9ePWfFBQAAigmHko3c3FxVq1ZNXbp0UWBgoFkxAQBQrOS5d2HDsQWinp6eeuyxx5SdnW1WPAAAFDt5sjjlKKoc3o3SokUL7d2714xYAAAolgwnHUWVw2s2Ro8erXHjxunkyZOKiIhQqVKl7K7ffffdTgsOAAAUfYVONoYOHao5c+booYcekiQ99dRTtmsWi0WGYchisejq1avOjxIAgCKMra+FtGzZMk2bNk2pqalmxgMAQLGT9wffKeYOCp1sGMa12aLQ0FDTggEAAMWPQ2s2/ujbXgEAQMGK8uJOZ3Ao2ahTp85NE47z58//qYAAAChuWLPhgClTpsjPz8+sWAAAQDHkULLRv39/VapUyaxYAAAoltz9DaKFTjZYrwEAwK0pym//dIZCv0H0+m4UAAAARxS6spGX5+7LWwAAuDXu/td1h19XDgAAHMOaDQAAYCp3nxtw+FtfAQAAHEGyAQCAyVz1FfM7duxQz549FRwcLIvFovj4eLvrFoulwOPf//63rU+HDh3yXe/fv79DcZBsAABgsjyLcw5HZWZmKiwsTK+99lqB10+dOmV3LF26VBaLRX379rXrN2LECLt+CxcudCgO1mwAAFBMRUVFKSoq6obXAwMD7c43bNigyMhI1ahRw67d19c3X19HUNkAAMBkeU46srOzlZ6ebndkZ2c7JcbTp0/rgw8+0LBhw/JdW7lypQICAtSwYUONHz9ev/32m0Njk2wAAGAyZyUbsbGx8vPzsztiY2OdEuOyZctUpkwZ9enTx6594MCBWrVqlRISEjRp0iStW7cuX5+bYRoFAIAiIiYmRmPHjrVrs1qtThl76dKlGjhwoHx8fOzaR4wYYfv3Ro0aqXbt2mrWrJn27Nmj8PDwQo1NsgEAgMkMJ73Uy2q1Oi25+F+fffaZDh8+rDVr1ty0b3h4uLy8vJSSkkKyAQDAneJOf6nXkiVLFBERobCwsJv2PXDggHJychQUFFTo8Uk2AAAopjIyMnTkyBHbeWpqqpKTk+Xv76+QkBBJUnp6ut59913NnDkz3/0//PCDVq5cqW7duikgIEAHDx7UuHHj1LRpU7Vp06bQcZBsAABgMldVNhITExUZGWk7v77eY9CgQYqLi5MkrV69WoZh6OGHH853v7e3tz755BPNnTtXGRkZqlq1qrp3767JkyfLw8Oj0HFYjGL43fF/q/agq0MA7khLft7p6hCAO07ulZ9Mf8arVR9xyjhPnljhlHFuNyobAACYzN2/9ZX3bAAAAFNR2QAAwGR3+m4Us5FsAABgMndPNphGAQAApqKyAQCAyYrdtk8HkWwAAGAydqMAAACYiMoGAAAmc/cFoiQbAACYzN3XbDCNAgAATEVlAwAAk+W5eW2DZAMAAJOxZgMAAJjKvesarNkAAAAmo7IBAIDJmEYBAACm4g2iAAAAJqKyAQCAydj6CgAATOXeqQbTKAAAwGRUNgAAMBm7UQAAgKncfc0G0ygAAMBUVDYAADCZe9c1SDYAADAdazYAAICpWLMBAABgIiobAACYzL3rGiQbAACYzt3XbDCNAgAATEVlAwAAkxluPpFCsgEAgMmYRgEAADARlQ0AAEzm7u/ZINkAAMBk7p1qMI0CAABMRmUDDqt9T311HtlLIY1rqFxlf80fOUP7tnxtu77w6LsF3rfu5eXa8uZGSVK7hzupee+2CmlYXSXL+GrM3YOUlX7ptsQPmKFd2xYaN+4xhTdtrODgQPV5YKg2bvyvXZ969Wop9uWJurddS5UoUUIHD36v/gP+phMnflb58uU0+flxuv/+9qpaJVjnzp3Xho2bNfmFfys9/TcXfSo4C9MogIO8fa06eeiYdr67XaMWTsh3fULzEXbnjTo00V+nP6Y9H+3+vzFKeuvAp8k68Gmy+jw70PSYAbOVKuWrb745qLhla7T2ncX5rteoEapPt8frrbhVmvLiK7p48TfVr1dbly9nS5KCgysrOLiynn32JR089L1CQ6ro9denKTg4UA/1H3m7Pw6czN13o5BswGEHEpJ1ICH5htfTz/5qdx52f3N9v+uAzp04Y2v7ZOmHkqQ6LRuYESJw223+73Zt/u/2G15/6cVn9dHmbfpHzFRbW2rqcdu/HzhwWP0e+r+k4scfj2nS89P1dtw8eXh46OrVq+YEjtvC3d+zwZoNmKpMgJ8aR4br8zXbXB0K4DIWi0XdojoqJeVHffj+Sv18cp92fr5JvXp1+cP7/MqWUXp6BokGirw7Otk4ceKEhg4d+od9srOzlZ6ebndcNfiDeado1be9Lmde1t7/funqUACXqVQpQGXKlNbfJzyu/25JUFT3AYrfsFlr31mse9u1LPAef//ymvjcGC1avOI2Rwsz5DnpKKru6GTj/PnzWrZs2R/2iY2NlZ+fn92x9+J3tylC3Eybfvfpq/jPlJud4+pQAJcpUeLa/2o3bvqv5s5bpH37DmjGv1/XBx9+rJEj/5qvf5kypbVpw9s6dOh7vfjSrNsdLkxgOOmfosqlycbGjRv/8Ni+/cbzn9fFxMTo4sWLdkdTv3q3IXrcTK3m9RRY8y59vuYTV4cCuNS5c+eVk5OjQ4dS7Nq/+y5FIVXvsmsrXbqUPnx/pTIyMtX3weHKzc29naGimNmxY4d69uyp4OBgWSwWxcfH2123WCwFHv/+979tfbKzs/Xkk08qICBApUqVUq9evXTy5EmH4nDpAtHo6GhZLBYZxo2zNYvF8odjWK1WWa1WuzYPi4dT4sOf0+ahjjr2zQ86eeiYq0MBXConJ0eJiftUp05Nu/batWvo2PH/+592mTKl9dEH/1F2drai+wxWdnb27Q4VJnHVFEhmZqbCwsI0ZMgQ9e3bN9/1U6dO2Z1/9NFHGjZsmF3fMWPGaNOmTVq9erUqVKigcePGqUePHkpKSpKHR+F+37o02QgKCtLrr7+u6OjoAq8nJycrIiLi9gaFm7L6+qhitUDbeUDVSqrSoJoyf83QhZ/PSZJ8SpdURLeWWjv17QLHKFuxnMpWLKeKodfGuatuiC5nXtb5n87p0sUM8z8E4GSlSvmqVq3qtvPq1UIUFtZQ589f0IkTP+uVWQu0auUCffbZbiV8ulNdOndQj+73q2OnByRdq2hs/nCVSvr66NHBT6ps2TIqW7aMJOns2V+Ul1eUZ+yR9wd/qTZTVFSUoqKibng9MDDQ7nzDhg2KjIxUjRo1JEkXL17UkiVLtHz5cnXq1EmStGLFClWtWlUff/yxunT540XO17k02YiIiNCePXtumGzcrOoB1wi9u4bGrZ5iO+83abAkaefaBC0b/7okqXnPNrJYLPpq4xcFjnHvwPvVc0w/2/mEd1+SJMWNf1271iaYEzhgomYRYfrk47W285mvvCBJWvb2Oxo2/Blt2LBZox//h579+5OaM/tFHf7+Rz340Ah9sfPaC/Eiwu9WixbhkqTvv9tpN3bN2i107JhjZWvAUadPn9YHH3xgt1YyKSlJOTk56ty5s60tODhYjRo10s6dO4tGsjFhwgRlZmbe8HqtWrUKtW4Dt9f3uw/qb9Ue/MM+n636WJ+t+viG19+f867en1Pwm0aBoujTHbvk6X3XH/aJW7ZGccvW3PL9KLqc9dfm7OzsfNNrBS0nuBXLli1TmTJl1KdPH1tbWlqavL29Vb58ebu+lStXVlpaWqHHdukC0Xbt2qlr1643vF6qVCm1b9/+NkYEAIDz5clwylHQDszY2FinxLh06VINHDhQPj4+N+1rGMZN11T+L94gCgBAERETE6OxY8fatTmjqvHZZ5/p8OHDWrPGvvIWGBioK1eu6MKFC3bVjTNnzqh169aFHv+Ofs8GAADFgbPes2G1WlW2bFm7wxnJxpIlSxQREaGwsDC79oiICHl5eWnr1q22tlOnTmn//v0OJRtUNgAAMJmr9hJlZGToyJEjtvPU1FQlJyfL399fISEhkqT09HS9++67mjlzZr77/fz8NGzYMI0bN04VKlSQv7+/xo8fr8aNG9t2pxQGyQYAACZz1VfMJyYmKjIy0nZ+fQpm0KBBiouLkyStXr1ahmHo4YcfLnCM2bNny9PTU/369VNWVpY6duyouLi4Qr9jQ5IsRjHcW3qznRKAu1ry886bdwLcTO6Vn0x/xoOhvZ0yzrvHNjhlnNuNygYAACYryt9r4gwkGwAAmMzd3//KbhQAAGAqKhsAAJisGC6PdAjJBgAAJnPVbpQ7BdMoAADAVFQ2AAAwmbsvECXZAADAZO6+9ZVpFAAAYCoqGwAAmMzdF4iSbAAAYDK2vgIAAFO5+wJR1mwAAABTUdkAAMBk7r4bhWQDAACTufsCUaZRAACAqahsAABgMnajAAAAUzGNAgAAYCIqGwAAmIzdKAAAwFR5br5mg2kUAABgKiobAACYzL3rGiQbAACYzt13o5BsAABgMndPNlizAQAATEVlAwAAk/EGUQAAYCqmUQAAAExEZQMAAJPxBlEAAGAqd1+zwTQKAAAwFZUNAABM5u4LREk2AAAwGdMoAAAAJqKyAQCAyZhGAQAApmLrKwAAMFUeazYAAADMQ2UDAACTMY0CAABMxTQKAACAiahsAABgMqZRAACAqZhGAQAAxdKOHTvUs2dPBQcHy2KxKD4+Pl+fQ4cOqVevXvLz81OZMmXUsmVLHT9+3Ha9Q4cOslgsdkf//v0dioNkAwAAkxlO+sdRmZmZCgsL02uvvVbg9R9++EFt27ZVvXr1lJCQoH379mnSpEny8fGx6zdixAidOnXKdixcuNChOJhGAQDAZK6aRomKilJUVNQNr0+cOFHdunXTjBkzbG01atTI18/X11eBgYG3HAeVDQAAiojs7Gylp6fbHdnZ2bc0Vl5enj744APVqVNHXbp0UaVKldSiRYsCp1pWrlypgIAANWzYUOPHj9dvv/3m0LNINgAAMJmzplFiY2Pl5+dnd8TGxt5STGfOnFFGRoamTZumrl27asuWLfrLX/6iPn366NNPP7X1GzhwoFatWqWEhARNmjRJ69atU58+fRx6lsUwit8S2b9Ve9DVIQB3pCU/73R1CMAdJ/fKT6Y/o3qFMKeM893PX+WrZFitVlmt1pvea7FYtH79ekVHR0uSfv75Z9111116+OGH9Z///MfWr1evXipVqpRWrVpV4DhJSUlq1qyZkpKSFB4eXqi4qWwAAGCyPBlOOaxWq8qWLWt3FCbRKEhAQIA8PT3VoEEDu/b69evb7Ub5vfDwcHl5eSklJaXQzyLZAADADXl7e6t58+Y6fPiwXfv333+v0NDQG9534MAB5eTkKCgoqNDPYjcKAAAmc9WKhYyMDB05csR2npqaquTkZPn7+yskJEQTJkzQQw89pHvvvVeRkZHavHmzNm3apISEBEnXtsauXLlS3bp1U0BAgA4ePKhx48apadOmatOmTaHjYM0G4EZYswHkdzvWbFTxb+SUcU6e3+9Q/4SEBEVGRuZrHzRokOLi4iRJS5cuVWxsrE6ePKm6detqypQp6t27tyTpxIkTeuSRR7R//35lZGSoatWq6t69uyZPnix/f/9Cx0GyAbgRkg0gv+KcbNwpmEYBAMBkxfDv9Q4h2QAAwGR8ERsAAICJqGwAAGCyW/kSteKEZAMAAJO5+5oNplEAAICpqGwAAGCyPKZRAACAmdx9GoVkAwAAk7H1FQAAwERUNgAAMBnTKAAAwFTuvkCUaRQAAGAqKhsAAJiMaRQAAGAqdqMAAACYiMoGAAAm44vYAACAqZhGAQAAMBGVDQAATMZuFAAAYCrWbAAAAFO5e2WDNRsAAMBUVDYAADCZu1c2SDYAADCZe6caTKMAAACTWQx3r+3ANNnZ2YqNjVVMTIysVqurwwHuGPzZgLsh2YBp0tPT5efnp4sXL6ps2bKuDge4Y/BnA+6GaRQAAGAqkg0AAGAqkg0AAGAqkg2Yxmq1avLkySyAA36HPxtwNywQBQAApqKyAQAATEWyAQAATEWyAQAATEWyAQAATEWyAdPMnz9f1atXl4+PjyIiIvTZZ5+5OiTApXbs2KGePXsqODhYFotF8fHxrg4JuC1INmCKNWvWaMyYMZo4caL27t2rdu3aKSoqSsePH3d1aIDLZGZmKiwsTK+99pqrQwFuK7a+whQtWrRQeHi4FixYYGurX7++oqOjFRsb68LIgDuDxWLR+vXrFR0d7epQANNR2YDTXblyRUlJSercubNde+fOnbVz504XRQUAcBWSDTjduXPndPXqVVWuXNmuvXLlykpLS3NRVAAAVyHZgGksFovduWEY+doAAMUfyQacLiAgQB4eHvmqGGfOnMlX7QAAFH8kG3A6b29vRUREaOvWrXbtW7duVevWrV0UFQDAVTxdHQCKp7Fjx+qvf/2rmjVrplatWunNN9/U8ePHNWrUKFeHBrhMRkaGjhw5YjtPTU1VcnKy/P39FRIS4sLIAHOx9RWmmT9/vmbMmKFTp06pUaNGmj17tu69915XhwW4TEJCgiIjI/O1Dxo0SHFxcbc/IOA2IdkAAACmYs0GAAAwFckGAAAwFckGAAAwFckGAAAwFckGAAAwFckGAAAwFckGAAAwFckGcAd54YUX1KRJE9v54MGDFR0dfdvjOHr0qCwWi5KTk2/Yp1q1apozZ06hx4yLi1O5cuX+dGwWi0Xx8fF/ehwAtw/JBnATgwcPlsVikcVikZeXl2rUqKHx48crMzPT9GfPnTu30G+WLEyCAACuwHejAIXQtWtXvfXWW8rJydFnn32m4cOHKzMzUwsWLMjXNycnR15eXk55rp+fn1PGAQBXorIBFILValVgYKCqVq2qAQMGaODAgbZS/vWpj6VLl6pGjRqyWq0yDEMXL17UyJEjValSJZUtW1b33Xef9u3bZzfutGnTVLlyZZUpU0bDhg3T5cuX7a7/fholLy9P06dPV61atWS1WhUSEqKpU6dKkqpXry5Jatq0qSwWizp06GC776233lL9+vXl4+OjevXqaf78+XbP+eqrr9S0aVP5+PioWbNm2rt3r8M/o1mzZqlx48YqVaqUqlatqtGjRysjIyNfv/j4eNWpU0c+Pj66//77deLECbvrmzZtUkREhHx8fFSjRg1NmTJFubm5DscD4M5BsgHcgpIlSyonJ8d2fuTIEb3zzjtat26dbRqje/fuSktL04cffqikpCSFh4erY8eOOn/+vCTpnXfe0eTJkzV16lQlJiYqKCgoXxLwezExMZo+fbomTZqkgwcP6j//+Y8qV64s6VrCIEkff/yxTp06pffee0+StGjRIk2cOFFTp07VoUOH9PLLL2vSpElatmyZJCkzM1M9evRQ3bp1lZSUpBdeeEHjx493+GdSokQJzZs3T/v379eyZcu0bds2/f3vf7frc+nSJU2dOlXLli3TF198ofT0dPXv3992/b///a8eeeQRPfXUUzp48KAWLlyouLg4W0IFoIgyAPyhQYMGGb1797adf/nll0aFChWMfv36GYZhGJMnTza8vLyMM2fO2Pp88sknRtmyZY3Lly/bjVWzZk1j4cKFhmEYRqtWrYxRo0bZXW/RooURFhZW4LPT09MNq9VqLFq0qMA4U1NTDUnG3r177dqrVq1q/Oc//7Fre+mll4xWrVoZhmEYCxcuNPz9/Y3MzEzb9QULFhQ41v8KDQ01Zs+efcPr77zzjlGhQgXb+VtvvWVIMnbv3m1rO3TokCHJ+PLLLw3DMIx27doZL7/8st04y5cvN4KCgmznkoz169ff8LkA7jys2QAK4f3331fp0qWVm5urnJwc9e7dW6+++qrtemhoqCpWrGg7T0pKUkZGhipUqGA3TlZWln744QdJ0qFDhzRq1Ci7661atdL27dsLjOHQoUPKzs5Wx44dCx332bNndeLECQ0bNkwjRoywtefm5trWgxw6dEhhYWHy9fW1i8NR27dv18svv6yDBw8qPT1dubm5unz5sjIzM1WqVClJkqenp5o1a2a7p169eipXrpwOHTqke+65R0lJSfr666/tKhlXr17V5cuXdenSJbsYARQdJBtAIURGRmrBggXy8vJScHBwvgWg13+ZXpeXl6egoCAlJCTkG+tWt3+WLFnS4Xvy8vIkXZtKadGihd01Dw8PSZJhGLcUz/86duyYunXrplGjRumll16Sv7+/Pv/8cw0bNsxuukm6tnX196635eXlacqUKerTp0++Pj4+Pn86TgCuQbIBFEKpUqVUq1atQvcPDw9XWlqaPD09Va1atQL71K9fX7t379ajjz5qa9u9e/cNx6xdu7ZKliypTz75RMOHD8933dvbW9K1SsB1lStX1l133aUff/xRAwcOLHDcBg0aaPny5crKyrIlNH8UR0ESExOVm5urmTNnqkSJa0vB3nnnnXz9cnNzlZiYqHvuuUeSdPjwYf3666+qV6+epGs/t8OHDzv0swZw5yPZAEzQqVMntWrVStHR0Zo+fbrq1q2rn3/+WR9++KGio6PVrFkzPf300xo0aJCaNWumtm3bauXKlTpw4IBq1KhR4Jg+Pj569tln9fe//13e3t5q06aNzp49qwMHDmjYsGGqVKmSSpYsqc2bN6tKlSry8fGRn5+fXnjhBT311FMqW7asoqKilJ2drcTERF24cEFjx47VgAEDNHHiRA0bNkz//Oc/dfToUb3yyisOfd6aNWsqNzdXr776qnr27KkvvvhCb7zxRr5+Xl5eevLJJzVv3jx5eXnpiSeeUMuWLW3Jx/PPP68ePXqoatWqevDBB1WiRAl98803+vbbb/Wvf/3L8f8QAO4I7EYBTGCxWPThhx/q3nvv1dChQ1WnTh31799fR48ete0eeeihh/T888/r2WefVUREhI4dO6bHHnvsD8edNGmSxo0bp+eff17169fXQw89pDNnzki6th5i3rx5WrhwoYKDg9W7d29J0vDhw7V48WLFxcWpcePGat++veLi4mxbZUuXLq1Nmzbp4MGDatq0qSZOnKjp06c79HmbNGmiWbNmafr06WrUqJFWrlyp2NjYfP18fX317LPPasCAAWrVqpVKliyp1atX26536dJF77//vrZu3armzZurZcuWmjVrlkJDQx2KB8CdxWI4Y8IWAADgBqhsAAAAU5FsAAAAU5FsAAAAU5FsAAAAU5FsAAAAU5FsAAAAU5FsAAAAU5FsAAAAU5FsAAAAU5FsAAAAU5FsAAAAU5FsAAAAU/0/tXfof5vvRbAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot confusion matrix for test data\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size = (150,150),\n",
    "    batch_size = 16,\n",
    "    class_mode = 'binary'\n",
    "\n",
    ")\n",
    "\n",
    "test_loss, test_acc = base_model.evaluate(test_generator)\n",
    "print('test acc:', test_acc)\n",
    "y_pred = base_model.predict(test_generator)\n",
    "\n",
    "y_pred = np.round(y_pred)\n",
    "\n",
    "cm = confusion_matrix(test_generator.classes, y_pred)\n",
    "cm\n",
    "\n",
    "test_acc, test_loss = base_model.evaluate(test_generator, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)\n",
    "print('\\nTest loss:', test_loss)\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='g')\n",
    "plt.title('Confusion matrix')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "\n",
    "#save confusion matrix\n",
    "plt.savefig('base_model_confusion_matrix.png')\n",
    "\n",
    "\n",
    "#print classification report\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_generator.classes, y_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrainned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benit\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\rmsprop.py:140: UserWarning:\n",
      "\n",
      "The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "32/32 [==============================] - 76s 2s/step - loss: 0.4590 - acc: 0.7734 - val_loss: 0.4302 - val_acc: 0.8105\n",
      "Epoch 2/30\n",
      "32/32 [==============================] - 74s 2s/step - loss: 0.3485 - acc: 0.8691 - val_loss: 0.5823 - val_acc: 0.7441\n",
      "Epoch 3/30\n",
      "32/32 [==============================] - 74s 2s/step - loss: 0.3354 - acc: 0.8555 - val_loss: 0.7003 - val_acc: 0.6738\n",
      "Epoch 4/30\n",
      "32/32 [==============================] - 74s 2s/step - loss: 0.3495 - acc: 0.8574 - val_loss: 0.6258 - val_acc: 0.7070\n",
      "Epoch 5/30\n",
      "19/32 [================>.............] - ETA: 15s - loss: 0.3560 - acc: 0.8328"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m'\u001b[39m, optimizer\u001b[39m=\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mRMSprop(lr\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m), metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39macc\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     18\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m history1 \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     20\u001b[0m     train_generator,\n\u001b[0;32m     21\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,\n\u001b[0;32m     22\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m,\n\u001b[0;32m     23\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_generator,\n\u001b[0;32m     24\u001b[0m     validation_steps\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[early_stopping]\n\u001b[0;32m     25\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\benit\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\benit\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\benit\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\benit\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\benit\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\benit\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\benit\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\benit\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\benit\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load pre-trained VGG16 model without the top layers (include_top=False)\n",
    "pr_model = VGG16(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
    "\n",
    "# Freeze the pre-trained layers to prevent them from being updated during training\n",
    "pr_model.trainable = False\n",
    "\n",
    "# Create a new model by adding your own classifier on top of the pre-trained base\n",
    "model = keras.models.Sequential()\n",
    "model.add(pr_model)\n",
    "model.add(keras.layers.GlobalAveragePooling2D())\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(128, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.RMSprop(lr=1e-3), metrics=['acc'])\n",
    "\n",
    "# Train the model\n",
    "history1 = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=32,\n",
    "    epochs=30,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=32, callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history1.history['acc']\n",
    "val_acc = history1.history['val_acc']\n",
    "loss = history1.history['loss']\n",
    "val_loss = history1.history['val_loss']\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Assuming you have defined the variables 'acc' and 'val_acc'\n",
    "\n",
    "#title\n",
    "fig.update_layout(\n",
    "    title=\"Pretrained Model\",\n",
    "    xaxis_title=\"Epochs\",\n",
    "    yaxis_title=\"Accuracy\",\n",
    "    font=dict(\n",
    "        family=\"Courier New, monospace\",\n",
    "        size=18,\n",
    "        color=\"#7f7f7f\"\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "fig.add_trace(go.Scatter(x=np.arange(1, len(acc)+1), y=acc,\n",
    "                    mode='lines',\n",
    "                    name='accuracy'))\n",
    "fig.add_trace(go.Scatter(x=np.arange(1, len(val_acc)+1), y=val_acc,\n",
    "                    mode='lines',\n",
    "                    name='validation accuracy'))\n",
    "\n",
    "# Save the plot as an HTML file\n",
    "pio.write_html(fig, 'pretrained_model.html')\n",
    "\n",
    "\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size = (150,150),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'binary'\n",
    ")\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print('test acc:', test_acc)\n",
    "y_pred = model.predict(test_generator)\n",
    "\n",
    "y_pred = np.round(y_pred)\n",
    "\n",
    "cm = confusion_matrix(test_generator.classes, y_pred)\n",
    "cm\n",
    "\n",
    "\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='g')\n",
    "plt.title('Confusion matrix')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "\n",
    "#save confusion matrix\n",
    "plt.savefig('base_model_confusion_matrix.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# 1st convolutional layer\n",
    "model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150,150,3)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "# 2nd convolutional layer\n",
    "model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "# 3rd convolutional layer\n",
    "model.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "# FFNN\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_5 (Conv2D)           (None, 148, 148, 32)      896       \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 148, 148, 32)     128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 74, 74, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 74, 74, 32)        0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 72, 72, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 72, 72, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 36, 36, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 36, 36, 64)        0         \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 34, 34, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 34, 34, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 17, 17, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 17, 17, 128)       0         \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 36992)             0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 512)               18940416  \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19,037,121\n",
      "Trainable params: 19,035,649\n",
      "Non-trainable params: 1,472\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "50/50 [==============================] - 35s 661ms/step - loss: 0.9089 - accuracy: 0.7200 - val_loss: 0.8924 - val_accuracy: 0.5785\n",
      "Epoch 2/15\n",
      "50/50 [==============================] - 28s 569ms/step - loss: 0.6218 - accuracy: 0.7760 - val_loss: 1.6174 - val_accuracy: 0.5785\n",
      "Epoch 3/15\n",
      "50/50 [==============================] - 28s 570ms/step - loss: 0.5760 - accuracy: 0.8000 - val_loss: 2.2947 - val_accuracy: 0.5785\n",
      "Epoch 4/15\n",
      "50/50 [==============================] - 29s 574ms/step - loss: 0.4482 - accuracy: 0.8360 - val_loss: 3.3709 - val_accuracy: 0.5785\n",
      "Epoch 5/15\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.4064 - accuracy: 0.8449"
     ]
    }
   ],
   "source": [
    "history2 = model.fit(train_generator, epochs=15, validation_data=validation_generator, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[92], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m, \u001b[39m5\u001b[39m))\n\u001b[0;32m      4\u001b[0m sns\u001b[39m.\u001b[39mset_style(\u001b[39m\"\u001b[39m\u001b[39mdarkgrid\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m plt\u001b[39m.\u001b[39mplot(history\u001b[39m.\u001b[39;49mhistory[\u001b[39m'\u001b[39;49m\u001b[39maccuracy\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m      6\u001b[0m plt\u001b[39m.\u001b[39mplot(history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      7\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m'\u001b[39m\u001b[39mModel 4 Training and Validation Accuracy 6000 Images with Data Augmentation and Batch Normalization\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'accuracy'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history2.history['accuracy']\n",
    "val_acc = history2.history['val_accuracy']\n",
    "loss = history2.history['loss']\n",
    "val_loss = history2.history['val_loss']\n",
    "\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Assuming you have defined the variables 'acc' and 'val_acc'\n",
    "\n",
    "#title\n",
    "fig.update_layout(\n",
    "    title=\"CNN Model\",\n",
    "    xaxis_title=\"Epochs\",\n",
    "    yaxis_title=\"Accuracy\",\n",
    "    font=dict(\n",
    "        family=\"Courier New, monospace\",\n",
    "        size=18,\n",
    "        color=\"#7f7f7f\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Scatter(x=np.arange(1, len(acc)+1), y=acc,\n",
    "                    mode='lines',\n",
    "                    name='accuracy'))\n",
    "fig.add_trace(go.Scatter(x=np.arange(1, len(val_acc)+1), y=val_acc,\n",
    "                    mode='lines',\n",
    "                    name='validation accuracy'))\n",
    "\n",
    "# Save the plot as an HTML file\n",
    "pio.write_html(fig, 'C_D_model.html')\n",
    "\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size = (150,150),\n",
    "    batch_size = 16,\n",
    "    class_mode = 'binary'\n",
    ")\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print('test acc:', test_acc)\n",
    "y_pred = model.predict(test_generator)\n",
    "\n",
    "y_pred = np.round(y_pred)\n",
    "\n",
    "cm = confusion_matrix(test_generator.classes, y_pred)\n",
    "cm\n",
    "\n",
    "\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='g')\n",
    "plt.title('Confusion matrix')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "\n",
    "#save confusion matrix\n",
    "plt.savefig('base_model_confusion_matrix.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 358 images belonging to 2 classes.\n",
      "18/18 [==============================] - 11s 615ms/step - loss: 1.1475 - accuracy: 0.7207\n",
      "test acc: 0.7206704020500183\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size = (150,150),\n",
    "    batch_size = 16,\n",
    "    class_mode = 'binary'\n",
    ")\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print('test acc:', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
